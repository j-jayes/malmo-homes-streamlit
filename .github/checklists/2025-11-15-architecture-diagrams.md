# Property Scraper Architecture - Visual Guide

## ğŸ”„ Complete Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HEMNET.SE (Source)                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   For-Sale (/bostad/) â”‚    â”‚   Sold (/salda/)      â”‚          â”‚
â”‚  â”‚   ~1,500 properties   â”‚    â”‚   ~500/month          â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 1: Link Collection (Working âœ…)                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  link_collector.py                                â”‚          â”‚
â”‚  â”‚  - Pagination handling                            â”‚          â”‚
â”‚  â”‚  - Cloudflare bypass                              â”‚          â”‚
â”‚  â”‚  - Rate limiting                                  â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                            â†“                                     â”‚
â”‚  Output: data/raw/links/active_20251115.csv                     â”‚
â”‚          property_id,url,found_at                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 2: Property Detail Scraping (To Build ğŸš§)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  property_detail_scraper.py                       â”‚          â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚          â”‚
â”‚  â”‚  â”‚  1. Detect Type (/bostad/ or /salda/)      â”‚ â”‚          â”‚
â”‚  â”‚  â”‚  2. Extract Common Fields (all properties) â”‚ â”‚          â”‚
â”‚  â”‚  â”‚  3. Extract Type-Specific Fields           â”‚ â”‚          â”‚
â”‚  â”‚  â”‚  4. Get Coordinates (Maps API)             â”‚ â”‚          â”‚
â”‚  â”‚  â”‚  5. Validate with Pydantic Schema          â”‚ â”‚          â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚          â”‚
â”‚  â”‚                                                   â”‚          â”‚
â”‚  â”‚  Features:                                        â”‚          â”‚
â”‚  â”‚  âœ“ Unified scraper (both types)                  â”‚          â”‚
â”‚  â”‚  âœ“ Batch processing (100 at a time)              â”‚          â”‚
â”‚  â”‚  âœ“ Resume capability                              â”‚          â”‚
â”‚  â”‚  âœ“ Parquet output                                 â”‚          â”‚
â”‚  â”‚  âœ“ Error handling + retries                      â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                            â†“                                     â”‚
â”‚  Output: data/raw/properties/active/20251115/                   â”‚
â”‚          â”œâ”€â”€ batch_000.parquet (100 properties, ~75 KB)         â”‚
â”‚          â”œâ”€â”€ batch_001.parquet                                  â”‚
â”‚          â””â”€â”€ metadata.json                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 3: DuckDB Storage (To Build ğŸš§)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  load_to_duckdb.py                                â”‚          â”‚
â”‚  â”‚  - Reads Parquet files                            â”‚          â”‚
â”‚  â”‚  - Deduplicates by property_id                    â”‚          â”‚
â”‚  â”‚  - Updates incrementally                          â”‚          â”‚
â”‚  â”‚  - Creates indexes                                â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                            â†“                                     â”‚
â”‚  Output: data/processed/hemnet.duckdb                           â”‚
â”‚          â””â”€ properties table (all data, queryable)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ANALYSIS & VISUALIZATION                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ Streamlit  â”‚  â”‚   Quarto   â”‚  â”‚   FastAPI   â”‚              â”‚
â”‚  â”‚ Dashboard  â”‚  â”‚   Reports  â”‚  â”‚     API     â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Data Schema Hierarchy

```
BaseProperty (Common to all)
â”œâ”€â”€ property_id: str
â”œâ”€â”€ property_type: 'for_sale' | 'sold'
â”œâ”€â”€ url: str
â”œâ”€â”€ scraped_at: datetime
â”‚
â”œâ”€â”€ Location
â”‚   â”œâ”€â”€ address: str
â”‚   â”œâ”€â”€ city: str
â”‚   â”œâ”€â”€ neighborhood: str
â”‚   â”œâ”€â”€ latitude: float
â”‚   â””â”€â”€ longitude: float
â”‚
â”œâ”€â”€ Property Details
â”‚   â”œâ”€â”€ housing_type: str
â”‚   â”œâ”€â”€ ownership_type: str
â”‚   â”œâ”€â”€ rooms: float
â”‚   â”œâ”€â”€ living_area: float
â”‚   â”œâ”€â”€ floor: str
â”‚   â”œâ”€â”€ has_elevator: bool
â”‚   â”œâ”€â”€ has_balcony: bool
â”‚   â”œâ”€â”€ building_year: int
â”‚   â””â”€â”€ energy_class: str
â”‚
â”œâ”€â”€ Association
â”‚   â”œâ”€â”€ association_name: str
â”‚   â”œâ”€â”€ association_fee: int
â”‚   â””â”€â”€ operating_cost: int
â”‚
â””â”€â”€ description: str

     â†“ Extends to â†“

ForSaleProperty             SoldProperty
â”œâ”€â”€ asking_price            â”œâ”€â”€ asking_price
â”œâ”€â”€ price_per_sqm           â”œâ”€â”€ final_price â­
â”œâ”€â”€ viewing_times           â”œâ”€â”€ price_change â­
â””â”€â”€ days_on_market          â”œâ”€â”€ price_change_pct â­
                            â”œâ”€â”€ price_per_sqm_final â­
                            â”œâ”€â”€ sold_date â­
                            â””â”€â”€ days_on_market
```

---

## ğŸ”§ Batch Processing Flow

```
Input: 1,500 property URLs
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Batch Manager             â”‚
â”‚  - Split into batches of 100â”‚
â”‚  - Check already scraped   â”‚
â”‚  - Calculate remaining     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Batch 0: URLs 0-99        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Scrape each URL     â”‚  â”‚
â”‚  â”‚  Validate with schemaâ”‚  â”‚
â”‚  â”‚  Collect results     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â†“                  â”‚
â”‚  Save: batch_000.parquet   â”‚
â”‚  Commit to Git âœ…          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Batch 1: URLs 100-199     â”‚
â”‚  [Same process]            â”‚
â”‚  Save: batch_001.parquet   â”‚
â”‚  Commit to Git âœ…          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
       [... continues ...]
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Batch 14: URLs 1400-1499  â”‚
â”‚  [Same process]            â”‚
â”‚  Save: batch_014.parquet   â”‚
â”‚  Commit to Git âœ…          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Update metadata.json      â”‚
â”‚  {                         â”‚
â”‚    "total": 1500,          â”‚
â”‚    "scraped": 1500,        â”‚
â”‚    "batches": 15,          â”‚
â”‚    "status": "complete"    â”‚
â”‚  }                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Benefits:**
- âœ… Each batch commits separately (no data loss)
- âœ… Can resume from any batch
- âœ… Small files (easy to work with)
- âœ… GitHub Actions friendly

---

## ğŸ—‚ï¸ File Organization

```
malmo-homes-streamlit/
â”‚
â”œâ”€â”€ .github/
â”‚   â”œâ”€â”€ checklists/
â”‚   â”‚   â”œâ”€â”€ 2025-11-15-PROJECT-STATUS.md
â”‚   â”‚   â”œâ”€â”€ 2025-11-15-unified-scraper-architecture.md
â”‚   â”‚   â”œâ”€â”€ 2025-11-15-property-scraper-implementation.md
â”‚   â”‚   â””â”€â”€ 2025-11-15-property-scraper-summary.md
â”‚   â”‚
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ scrape_weekly.yml         (Active properties)
â”‚       â”œâ”€â”€ scrape_sold_monthly.yml   (Sold properties)
â”‚       â””â”€â”€ generate_reports.yml      (Quarto reports)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ links/                     ğŸ“„ PHASE 1: URLs only
â”‚   â”‚   â”‚   â”œâ”€â”€ active_20251115.csv
â”‚   â”‚   â”‚   â””â”€â”€ sold_202511.csv
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ properties/                ğŸ“¦ PHASE 2: Full details
â”‚   â”‚       â”œâ”€â”€ active/
â”‚   â”‚       â”‚   â””â”€â”€ 20251115/
â”‚   â”‚       â”‚       â”œâ”€â”€ batch_000.parquet
â”‚   â”‚       â”‚       â”œâ”€â”€ batch_001.parquet
â”‚   â”‚       â”‚       â””â”€â”€ metadata.json
â”‚   â”‚       â””â”€â”€ sold/
â”‚   â”‚           â””â”€â”€ 202511/
â”‚   â”‚               â”œâ”€â”€ batch_000.parquet
â”‚   â”‚               â””â”€â”€ metadata.json
â”‚   â”‚
â”‚   â””â”€â”€ processed/                     ğŸ—„ï¸ PHASE 3: Database
â”‚       â””â”€â”€ hemnet.duckdb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ property_schema.py         ğŸ†• Pydantic models
â”‚   â”‚
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”œâ”€â”€ link_collector.py          âœ… Working
â”‚   â”‚   â”œâ”€â”€ sold_properties_scraper.py âœ… Working
â”‚   â”‚   â””â”€â”€ property_detail_scraper.py ğŸš§ To build
â”‚   â”‚
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ database_schema.sql        ğŸš§ To build
â”‚   â”‚   â””â”€â”€ load_to_duckdb.py          ğŸš§ To build
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ batch_manager.py           ğŸš§ To build
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_property_schema.py        ğŸš§ To build
â”‚   â”œâ”€â”€ test_property_scraper.py       ğŸš§ To build
â”‚   â””â”€â”€ test_batch_manager.py          ğŸš§ To build
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ query_examples.ipynb           ğŸš§ To build
â”‚
â””â”€â”€ pyproject.toml                     (Add: pyarrow, duckdb)
```

---

## ğŸ”€ Property Type Detection

```python
def detect_property_type(url: str) -> str:
    """
    Automatically detect property type from URL
    """
    
    # For-sale properties
    if '/bostad/' in url:
        return 'for_sale'
    
    # Sold properties
    elif '/salda/' in url:
        return 'sold'
    
    else:
        raise ValueError(f"Unknown property type in URL: {url}")

# Examples:
url1 = "https://www.hemnet.se/bostad/lagenhet-2rum-..."
detect_property_type(url1)  # â†’ 'for_sale'

url2 = "https://www.hemnet.se/salda/lagenhet-3rum-..."
detect_property_type(url2)  # â†’ 'sold'
```

---

## ğŸ“Š Storage Size Comparison

```
                CSV        Parquet      Compression
Property        ~2 KB      ~200 bytes   10x smaller
Batch (100)     ~200 KB    ~20 KB       10x smaller
Weekly (1,500)  ~3 MB      ~300 KB      10x smaller
Annual          ~150 MB    ~15 MB       10x smaller

âœ… Parquet wins by 10x!
```

**Why it matters for GitHub Actions:**
- Faster uploads/downloads
- Less storage used
- Faster to query
- Better for Git commits

---

## ğŸ”„ Resume Capability

```
Scenario: Scraping 1,500 properties, crash at 250

Before Resume:
data/raw/properties/active/20251115/
â”œâ”€â”€ batch_000.parquet  âœ… (100 properties)
â”œâ”€â”€ batch_001.parquet  âœ… (100 properties)
â”œâ”€â”€ batch_002.parquet  âš ï¸ (50 properties - incomplete)
â””â”€â”€ metadata.json      (status: "in_progress", scraped: 250)

Run Again:
1. Read metadata.json â†’ See 250 done
2. Read scraped_ids.txt â†’ Get list of IDs
3. Load all URLs â†’ 1,500 total
4. Filter out already scraped â†’ 1,250 remaining
5. Continue from batch_003

After Resume:
â”œâ”€â”€ batch_000.parquet  âœ… (kept)
â”œâ”€â”€ batch_001.parquet  âœ… (kept)
â”œâ”€â”€ batch_002.parquet  âœ… (completed 50 â†’ 100)
â”œâ”€â”€ batch_003.parquet  âœ… (new)
â”œâ”€â”€ batch_004.parquet  âœ… (new)
...
â””â”€â”€ metadata.json      (status: "complete", scraped: 1500)
```

**No data is lost!** ğŸ‰

---

## ğŸ§ª Testing Pyramid

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  E2E Test   â”‚  â† Full pipeline (1 test)
                    â”‚  100 props  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†‘
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Integration Tests   â”‚  â† Multiple components
              â”‚  - Batch processing  â”‚     (3-5 tests)
              â”‚  - DuckDB loading    â”‚
              â”‚  - Resume logic      â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†‘
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚        Unit Tests                â”‚  â† Individual functions
         â”‚  - Schema validation             â”‚     (20+ tests)
         â”‚  - Type detection                â”‚
         â”‚  - Field extraction              â”‚
         â”‚  - Coordinate parsing            â”‚
         â”‚  - Parquet save/load             â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Test Coverage Goal:** >80%

---

## ğŸš€ GitHub Actions Workflow

```yaml
name: Weekly Property Scraping

on:
  schedule:
    - cron: '0 0 * * 0'  # Sunday midnight

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      # 1ï¸âƒ£ Collect Links
      - name: Collect property links
        run: python src/scrapers/link_collector.py
        
      # Output: data/raw/links/active_20251115.csv (1,500 URLs)
      
      # 2ï¸âƒ£ Scrape in Batches (loop through 15 batches)
      - name: Scrape batch 0
        run: |
          python src/scrapers/property_detail_scraper.py \
            --input data/raw/links/active_20251115.csv \
            --batch-start 0 --batch-end 99 \
            --output-dir data/raw/properties/active/20251115
      
      - name: Commit batch 0
        run: |
          git add data/raw/properties/
          git commit -m "Add batch 0 (100 properties)"
          git push
      
      # Repeat for batches 1-14...
      
      # 3ï¸âƒ£ Update Database
      - name: Load to DuckDB
        run: |
          python src/data/load_to_duckdb.py \
            --input-dir data/raw/properties/active/20251115
      
      - name: Commit database
        run: |
          git add data/processed/hemnet.duckdb
          git commit -m "Update database with new properties"
          git push
```

**Key Features:**
- âœ… Runs automatically every Sunday
- âœ… Each batch commits separately
- âœ… Can resume if interrupted
- âœ… Updates database at end
- âœ… Sends notification on failure

---

## ğŸ“ˆ Expected Performance

```
Metric                    Target      Reality Check
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Time per property         <30s        Realistic âœ…
Batch of 100 properties   <50 min     Achievable âœ…
Weekly scrape (1,500)     <12 hours   Within limits âœ…
Monthly scrape (500)      <4 hours    Easy âœ…
Parquet file size         <100 KB     Tested âœ…
Coordinate success rate   >95%        Historical âœ…
Schema validation         100%        Pydantic âœ…
```

**GitHub Actions Limits:**
- Free tier: 2,000 minutes/month
- Our usage: ~48 hours/month (weekly) + ~4 hours/month (monthly)
- Total: ~52 hours = **Well within limits!** âœ…

---

## ğŸ¯ Success Checklist

After implementation, verify:

- [ ] âœ… Both property types scraped correctly
- [ ] âœ… All fields extracted (>95% success)
- [ ] âœ… Coordinates found for all properties
- [ ] âœ… Parquet files <100 KB each
- [ ] âœ… Resume works after interruption
- [ ] âœ… DuckDB loads without errors
- [ ] âœ… Queries return correct results
- [ ] âœ… No duplicates in database
- [ ] âœ… GitHub Actions runs successfully
- [ ] âœ… All tests pass

---

## ğŸ’¡ Design Philosophy

**Principles we follow:**

1. **Separation of Concerns**
   - Link collection â‰  Property scraping
   - Each phase can be run independently
   - Easier to debug and test

2. **Fail-Safe Design**
   - Batch commits (no data loss)
   - Resume capability
   - Multiple extraction methods (fallbacks)
   - Comprehensive error handling

3. **Git-Friendly Storage**
   - Small files (easy to diff)
   - Immutable batches (append-only)
   - Clear directory structure
   - Metadata for tracking

4. **Future-Proof**
   - Extensible schema
   - Multiple property types supported
   - Easy to add new fields
   - Database-ready format

---

**This architecture will serve us for years to come!** ğŸš€
